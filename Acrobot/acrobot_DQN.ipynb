{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1452518-1d90-410f-9881-bf9cb1f7bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "env = gym.make(\"Acrobot-v1\", render_mode = \"human\")\n",
    "\n",
    "\"\"\" theta_1 = absolute angle of first link\n",
    "    theta_2 = angle of second link in relation to first link\n",
    "    state: [cos(theta_1), sin(theta_1), cos(theta_2), sin(theta_2), omega of theta_1, omega of theta_2]\n",
    "    action: [left, nothing, right]\n",
    "    left is minus, right is plus \n",
    "\n",
    "    Constant -1 reward at every step; 0 at termination\n",
    "    Termination condition: Free end reaches target height (-cos(theta1) - cos(theta2 + theta1) > 1.0)\n",
    "    Truncation condition: Episode length > 500\n",
    "    \n",
    "    Job of agent is to reach target height in as little steps as possible.\n",
    "\"\"\"\n",
    "\n",
    "def generate_episodes(n_ep, env):\n",
    "    \n",
    "    for _ in range(n_ep):\n",
    "        observation, info = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        while True:\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            \n",
    "            total_reward += reward\n",
    "            # print(total_reward)\n",
    "            \n",
    "            if terminated or total_reward < -99:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f966423-131c-4fe0-a337-86b75f924bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(env, Q_net, S, epsilon):\n",
    "    r = torch.rand(1)\n",
    "    if r > epsilon:\n",
    "        return torch.argmax(Q_net.forward(S))\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]])\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim) -> None:\n",
    "        super().__init__()\n",
    "        # note to self: nn.Linear() represents the transformation, not the matrices themselves.\n",
    "        self.il = nn.Linear(in_dim, 200)\n",
    "        self.hl = nn.Linear(200, 200)\n",
    "        self.ol = nn.Linear(200, out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.il(x))\n",
    "        x = F.relu(self.hl(x))\n",
    "        return self.ol(x)\n",
    "        \n",
    "class Replay_Memory():\n",
    "    def __init__(self, cap):\n",
    "        self.memory = deque([], maxlen = cap)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        mem_sample = random.sample(self.memory, batch_size)\n",
    "    \n",
    "        states, actions, rewards, next_states, dones = zip(*mem_sample)\n",
    "        \n",
    "        return (torch.stack(states), torch.stack(actions), torch.stack(rewards), \n",
    "                torch.stack(next_states), torch.stack(dones))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "def DQL(_env, n_ep):\n",
    "    state_dim = _env.observation_space.shape[0]\n",
    "    action_dim = _env.action_space.n\n",
    "    buffer_cap = 100000\n",
    "    \n",
    "    replay_buffer = Replay_Memory(buffer_cap)\n",
    "    behaviour_net = DQN(state_dim, action_dim).to(device)\n",
    "    target_net = DQN(state_dim, action_dim).to(device)\n",
    "\n",
    "    cumulative_reward_list = []\n",
    "    \n",
    "    target_net.load_state_dict(behaviour_net.state_dict())\n",
    "    target_net.eval()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(behaviour_net.parameters(), lr=1e-3)\n",
    "    loss_func = nn.MSELoss();\n",
    "\n",
    "    n_steps = 0\n",
    "    decay_min = 0.01\n",
    "    decay_const = 0.001\n",
    "    successes = 0\n",
    "    _gamma = 0.99\n",
    "\n",
    "    for i in range(n_ep):\n",
    "        S, info = _env.reset()\n",
    "        S = torch.tensor(S, dtype=torch.float32, device=device)\n",
    "        done = False\n",
    "        while not done:\n",
    "            epsilon = decay_min + (1 - decay_min) * np.exp(-decay_const * i)\n",
    "            \n",
    "            A = int(epsilon_greedy(env, behaviour_net, S, epsilon))\n",
    "            S_prime, R, terminated, truncated, info = _env.step(A)\n",
    "            done = terminated or truncated\n",
    "            n_steps += 1\n",
    "\n",
    "            S_prime = torch.tensor(S_prime, dtype=torch.float32, device=device)\n",
    "            A = torch.tensor([A], dtype=torch.int64, device=device) # Use int64 for indexing\n",
    "            R = torch.tensor([R], dtype=torch.float32, device=device)\n",
    "            done = torch.tensor([done], dtype=torch.float32, device=device)\n",
    "            \n",
    "            replay_buffer.push(S, A, R, S_prime, done)\n",
    "\n",
    "            S = S_prime\n",
    "\n",
    "            if terminated:\n",
    "                print(\"Episode\", i + 1)\n",
    "                print(\"success!\")\n",
    "                \n",
    "            if len(replay_buffer) > 64:\n",
    "                # sample from replay buffer\n",
    "                state_batch, action_batch, reward_batch, next_state_batch, done_batch = replay_buffer.sample(64)\n",
    "                # feed these values to Q-network, get the Q-values\n",
    "                \n",
    "                q_values = behaviour_net(state_batch)\n",
    "                # gather values from q_values with the index being action_batch values\n",
    "                specific_qs = q_values.gather(1, action_batch)\n",
    "\n",
    "                # get max q-value from target net\n",
    "                with torch.no_grad():\n",
    "                    specific_target_qs = target_net(next_state_batch).max(1)[0].unsqueeze(1)\n",
    "\n",
    "                # calculate target q-value\n",
    "                target_q_values = reward_batch + (_gamma * specific_target_qs * (1 - done_batch))            \n",
    "\n",
    "                #loss\n",
    "                loss = loss_func(specific_qs, target_q_values)\n",
    "\n",
    "                #backprop\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if (n_steps % 100) == 0:\n",
    "                target_net.load_state_dict(behaviour_net.state_dict())\n",
    "                \n",
    "    return behaviour_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b7a2fb4-fd54-4f72-8fb4-becfb4c3b698",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Acrobot-v1\")\n",
    "\n",
    "Q_net = DQL(env, 100)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b750c373-c2af-4e0b-a247-1bf1aea98401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the neural net\n",
    "\n",
    "# torch.save(Q_net.state_dict(), \"models/acrobot_DQN_100_iters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "74ec4191-2a64-4cab-bb40-1f9531f3579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "def greedy(Q_net, S):\n",
    "    return torch.argmax(Q_net.forward(S))\n",
    "    \n",
    "def generate_episode(policy, _env, Q_net):\n",
    "    observation, info = _env.reset()\n",
    "    observation = torch.tensor(observation, dtype=torch.float32, device=device)\n",
    "\n",
    "    while True:\n",
    "        action = int(greedy(Q_net, observation))\n",
    "        observation, reward, terminated, truncated, info = _env.step(action)\n",
    "        observation = torch.tensor(observation, dtype=torch.float32, device=device)\n",
    "        \n",
    "        if terminated or truncated or reward < -100:\n",
    "            break\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "Q_net_load = DQN(state_dim, action_dim).to(device)\n",
    "Q_net_load.load_state_dict(torch.load(\"models/acrobot_DQN_100_iters\", weights_only=True))\n",
    "Q_net_load.eval()\n",
    "\n",
    "env = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\")\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "env = RecordVideo(env, video_folder=\"./videos\")\n",
    "\n",
    "for _ in range(1):\n",
    "    generate_episode(greedy, env, Q_net_load)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84d9306-0857-44be-bb50-eb4b9d4f4770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
