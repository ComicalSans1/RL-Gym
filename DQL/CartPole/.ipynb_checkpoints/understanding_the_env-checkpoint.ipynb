{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d2fa58d8-d8ad-4b8c-ba59-b5fe1db4e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "\"\"\" state: [position, velocity, angle, radial velocity]\n",
    "action: [left, right]\n",
    "left is minus, right is plus\"\"\"\n",
    "\n",
    "def generate_episodes(n_ep, env):\n",
    "    for _ in range(n_ep):\n",
    "        observation, info = env.reset()\n",
    "\n",
    "        while True:\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            # print(observation, action)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ef42c013-0ebf-47b2-a7d3-ae829e1581be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_policy(state):\n",
    "    return int(state[2] < 0)\n",
    "\n",
    "def less_simple_policy(state):\n",
    "    r = np.random.rand()\n",
    "    threshold = 0.5\n",
    "    if np.absolute(state[2]) > 0.12:\n",
    "        return int(state[2] < 0)\n",
    "    else:\n",
    "        return np.random.randint(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "426f1030-b64e-4e98-89b3-6633c7223091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.756 19.942\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "def test_policy(n_ep, policy, _env):\n",
    "    reward_list = []\n",
    "    \n",
    "    for _ in range(n_ep):\n",
    "        reward_count = 0\n",
    "        observation, info = _env.reset()\n",
    "        \n",
    "        while True:\n",
    "            action = policy(observation)\n",
    "            observation, reward, terminated, truncated, info = _env.step(action)\n",
    "            reward_count += reward\n",
    "            # print(observation, action)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        reward_list.append(reward_count)\n",
    "    return np.mean(reward_list)\n",
    "\n",
    "avg_reward_1 = test_policy(1000, simple_policy, env)\n",
    "avg_reward_2 = test_policy(1000, less_simple_policy, env)\n",
    "\n",
    "print(avg_reward_1, avg_reward_2)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "44477010-1156-49dd-ac62-3a54d1cf21e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, info = env.reset()\n",
    "action = env.action_space.sample()\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "16070b5a-ce3e-4fa5-a567-69c88e4ae136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(torch.tensor([1, 100, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f07b1a75-0752-413f-b146-242a1421644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(env, Q_net, S, epsilon):\n",
    "    r = torch.rand(1)\n",
    "    if r > epsilon:\n",
    "        return torch.argmax(Q_net.forward(S))\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]])\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim) -> None:\n",
    "        super().__init__()\n",
    "        # note to self: nn.Linear() represents the transformation, not the matrices themselves.\n",
    "        self.il = nn.Linear(in_dim, 50)\n",
    "        self.ol = nn.Linear(50, out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.il(x))\n",
    "        return F.relu(self.ol(x))\n",
    "        \n",
    "class Replay_Memory():\n",
    "    def __init__(self, cap):\n",
    "        self.memory = deque([], maxlen = cap)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        mem_sample = random.sample(self.memory, batch_size)\n",
    "    \n",
    "        states, actions, rewards, next_states, dones = zip(*mem_sample)\n",
    "        \n",
    "        return (torch.stack(states), torch.stack(actions), torch.stack(rewards), \n",
    "                torch.stack(next_states), torch.stack(dones))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "def DQL(_env, n_ep):\n",
    "    state_dim = _env.observation_space.shape[0]\n",
    "    action_dim = _env.action_space.n\n",
    "    buffer_cap = 100000\n",
    "    \n",
    "    replay_buffer = Replay_Memory(buffer_cap)\n",
    "    behaviour_net = DQN(state_dim, action_dim)\n",
    "    target_net = DQN(state_dim, action_dim)\n",
    "\n",
    "    target_net.load_state_dict(behaviour_net.state_dict())\n",
    "    target_net.eval()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(behaviour_net.parameters(), lr=0.01)\n",
    "    loss_func = nn.MSELoss();\n",
    "\n",
    "    n_steps = 0\n",
    "    decay_min = 0.01\n",
    "    decay_const = 0.01\n",
    "    successes = 0\n",
    "    _gamma = 0.99\n",
    "\n",
    "    for i in range(n_ep):\n",
    "        S, info = _env.reset()\n",
    "        S = torch.tensor(S, dtype=torch.float32)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            epsilon = decay_min + (1 - decay_min) * np.exp(-decay_const * i)\n",
    "            \n",
    "            A = int(epsilon_greedy(env, behaviour_net, S, epsilon))\n",
    "            S_prime, R, terminated, truncated, info = _env.step(A)\n",
    "            done = terminated or truncated\n",
    "            n_steps += 1\n",
    "\n",
    "            S_prime = torch.tensor(S_prime, dtype=torch.float32)\n",
    "            A = torch.tensor([A], dtype=torch.int64) # Use int64 for indexing\n",
    "            R = torch.tensor([R], dtype=torch.float32)\n",
    "            done = torch.tensor([done], dtype=torch.float32)\n",
    "            \n",
    "            replay_buffer.push(S, A, R, S_prime, done)\n",
    "\n",
    "            S = S_prime\n",
    "            \n",
    "            if len(replay_buffer) > 64:\n",
    "                # sample from replay buffer\n",
    "                state_batch, action_batch, reward_batch, next_state_batch, done_batch = replay_buffer.sample(64)\n",
    "                # feed these values to Q-network, get the Q-values\n",
    "                \n",
    "                q_values = behaviour_net(state_batch)\n",
    "                # gather values from q_values with the index being action_batch values\n",
    "                specific_qs = q_values.gather(1, action_batch)\n",
    "\n",
    "                # get max q-value from target net\n",
    "                with torch.no_grad():\n",
    "                    specific_target_qs = target_net(next_state_batch).max(1)[0].unsqueeze(1)\n",
    "\n",
    "                # calculate target q-value\n",
    "                target_q_values = reward_batch + (_gamma * specific_target_qs * (1 - done_batch))            \n",
    "\n",
    "                #loss\n",
    "                loss = loss_func(specific_qs, target_q_values)\n",
    "\n",
    "                #backprop\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                    \n",
    "            if (n_steps % 100) == 0:\n",
    "                target_net.load_state_dict(behaviour_net.state_dict())\n",
    "                \n",
    "    return behaviour_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "2af03cc3-3f50-4833-8190-330cda551fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-3.1810,  0.1518,  0.3691, -0.6375]],\n",
      "\n",
      "        [[-3.1810,  0.1518,  0.3691, -0.6375]],\n",
      "\n",
      "        [[-3.1810,  0.1518,  0.3691, -0.6375]],\n",
      "\n",
      "        [[-3.1810,  0.1518,  0.3691, -0.6375]]])\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<GatherBackward0>)\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "example_buffer = [(torch.tensor(env.observation_space.sample()), torch.tensor(env.action_space.sample()), torch.tensor(1), torch.tensor(env.observation_space.sample()), torch.tensor(False))] * 4 \n",
    "example_buffer\n",
    "mem_sample = random.sample(example_buffer, 4)\n",
    "a, b, c, d, e = zip(*mem_sample)\n",
    "please = torch.stack(a)\n",
    "please_b = torch.stack(b)\n",
    "# print(please)\n",
    "print(please.unsqueeze(1))\n",
    "print(please_b.unsqueeze(1))\n",
    "\n",
    "example_net = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "ex_values = example_net(please)\n",
    "print(ex_values.gather(1, please_b.unsqueeze(1)))\n",
    "print(ex_values.gather(1, please_b.unsqueeze(1)).squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a3f7859a-0aa6-466b-ae6b-ee7c4c65a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_net = DQL(env, 100)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b76068-7094-47b0-a860-a6d54b492d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
