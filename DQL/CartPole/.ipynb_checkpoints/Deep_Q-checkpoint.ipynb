{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a8da382-868e-4a1e-9394-d25fec9fa61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # finally get to use pytorch again\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1f7a7ac-7110-4aa2-bc08-d27094324af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "observation, info = env.reset()\n",
    "action = env.action_space.sample()\n",
    "\n",
    "def generate_episode(n_ep, env):\n",
    "    for _ in range(n_ep):\n",
    "        observation, info = env.reset()\n",
    "\n",
    "        while True:\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef0738a8-d816-4390-ae34-d6b9be023399",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_network(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim) -> None:\n",
    "        super().__init__()\n",
    "        # note to self: nn.Linear() represents the transformation, not the matrices themselves.\n",
    "        self.il = nn.Linear(in_dim, 50)\n",
    "        self.ol = nn.Linear(50, out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.il(x))\n",
    "        return F.relu(self.ol(x))\n",
    "\n",
    "# shamelessly copied off of \n",
    "class Replay_Memory():\n",
    "    def __init__(self, cap):\n",
    "        self.memory = deque([], maxlen = cap)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(*args)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return np.random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "485cd269-4ff6-46e3-87ac-ffc4a43eb889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(Q_net, S, epsilon):\n",
    "    r = torch.rand(1)\n",
    "    if r > epsilon:\n",
    "        return torch.argmax(Q_net.forward(S))\n",
    "    else:\n",
    "        return random.randint(0, 1)\n",
    "\n",
    "def DQL(_env, n_ep):\n",
    "    state_dim = _env.observation_space.shape[0]\n",
    "    action_dim = _env.action_space.n\n",
    "    \n",
    "    buffer_cap = 100000\n",
    "    replay_buffer = Replay_Memory(buffer_cap)\n",
    "    \n",
    "    behaviour_net = Q_network(state_dim, action_dim)\n",
    "    target_net = Q_network(state_dim, action_dim)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(behaviour_net.parameters(), lr=0.01)\n",
    "    loss_func = nn.MSELoss();\n",
    "    \n",
    "    decay_min = 0.01\n",
    "    decay_const = 0.01\n",
    "    \n",
    "    successes = 0\n",
    "    _gamma = 0.99\n",
    "    # _alpha = 0.5\n",
    "\n",
    "    for i in range(n_ep):\n",
    "        epsilon = decay_min + (1 - decay_min) * np.exp(-decay_const * i)\n",
    "        observation, info = _env.reset()\n",
    "        S = observation\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            A = epsilon_greedy(behaviour_net, S, epsilon)\n",
    "            S_prime, R, terminated, truncated, info = _env.step(A)\n",
    "            done = terminated or truncated\n",
    "            replay_buffer.push(S, A, R, S_prime, done)\n",
    "\n",
    "            S = S_prime\n",
    "            \n",
    "            if len(replay_buffer) > 64:\n",
    "                # sample from replay buffer\n",
    "                state_batch, action_batch, reward_batch, next_state_batch, done_batch = replay_buffer.sample(64)\n",
    "                # feed these values to Q-network, get the Q-values\n",
    "                q_values = behaviour_net(state_batch)\n",
    "                # do magic shit to get Q-value for the action taken\n",
    "                specific_qs = q_values.gather(1, action_batch.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                # get max q-value from target net\n",
    "                with torch.no_grad():\n",
    "                    specific_target_qs = target_net(next_state_batch).max(1)[0]\n",
    "\n",
    "                # calculate target q-value\n",
    "                target_q_values = reward_batch + _gamma * specific_target_qs * (1 - done_batch)                \n",
    "\n",
    "                #loss\n",
    "                loss = loss_func(specific_qs, specific_target_qs)\n",
    "\n",
    "                #backprop\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            if (i % 500) == 0:\n",
    "                target_net.load_state_dict(behaviour_net.state_dict())\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                if R == 1:\n",
    "                    successes += 1\n",
    "                break\n",
    "                \n",
    "    print(\"Number of successes:\", successes)\n",
    "    return behaviour_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad12e329-dd16-43f0-ad05-ac21e0008ce1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.random' has no attribute 'choice'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Q_net \u001b[38;5;241m=\u001b[39m \u001b[43mDQL\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[7], line 35\u001b[0m, in \u001b[0;36mDQL\u001b[1;34m(_env, n_ep)\u001b[0m\n\u001b[0;32m     32\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 35\u001b[0m     A \u001b[38;5;241m=\u001b[39m \u001b[43mepsilon_greedy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbehaviour_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     S_prime, R, terminated, truncated, info \u001b[38;5;241m=\u001b[39m _env\u001b[38;5;241m.\u001b[39mstep(A)\n\u001b[0;32m     37\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m, in \u001b[0;36mepsilon_greedy\u001b[1;34m(Q_net, S, epsilon)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39margmax(Q_net\u001b[38;5;241m.\u001b[39mforward(S))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.random' has no attribute 'choice'"
     ]
    }
   ],
   "source": [
    "Q_net = DQL(env, 1000)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c6f664-1d78-4dab-a6bf-e3ccae1d25fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
