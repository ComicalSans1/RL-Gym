{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0a8da382-868e-4a1e-9394-d25fec9fa61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # finally get to use pytorch again\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b1f7a7ac-7110-4aa2-bc08-d27094324af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "action = env.action_space.sample()\n",
    "\n",
    "def generate_episode(n_ep, env):\n",
    "    for _ in range(n_ep):\n",
    "        observation, info = env.reset()\n",
    "\n",
    "        while True:\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "485cd269-4ff6-46e3-87ac-ffc4a43eb889",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_network(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim) -> None:\n",
    "        super().__init__()\n",
    "        # note to self: nn.Linear() represents the transformation, not the matrices themselves.\n",
    "        self.il = nn.Linear(in_dim, 50)\n",
    "        self.ol = nn.Linear(50, out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.tensor(x)\n",
    "        x = F.relu(self.il(x))\n",
    "        return F.relu(self.ol(x))\n",
    " \n",
    "class Replay_Memory():\n",
    "    def __init__(self, cap):\n",
    "        self.memory = deque([], maxlen = cap)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        mem_sample = random.sample(self.memory, batch_size)\n",
    "\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*mem_sample)\n",
    "\n",
    "        state_batch = torch.from_numpy(np.array(state_batch)).float()\n",
    "        action_batch = torch.tensor(action_batch).long()\n",
    "        reward_batch = torch.tensor(reward_batch).float()\n",
    "        next_state_batch = torch.from_numpy(np.array(next_state_batch)).float()\n",
    "        done_batch = torch.tensor(done_batch).float()\n",
    "\n",
    "        return [state_batch, action_batch, reward_batch, next_state_batch, done_batch]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "def epsilon_greedy(Q_net, S, epsilon):\n",
    "    r = torch.rand(1)\n",
    "    if r > epsilon:\n",
    "        return int(torch.argmax(Q_net.forward(S)).item())\n",
    "    else:\n",
    "        return random.randint(0, 1)\n",
    "\n",
    "def DQL(_env, n_ep):\n",
    "    state_dim = _env.observation_space.shape[0]\n",
    "    action_dim = _env.action_space.n\n",
    "    \n",
    "    buffer_cap = 100000\n",
    "    replay_buffer = Replay_Memory(buffer_cap)\n",
    "    \n",
    "    behaviour_net = Q_network(state_dim, action_dim)\n",
    "    target_net = Q_network(state_dim, action_dim)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(behaviour_net.parameters(), lr=0.01)\n",
    "    loss_func = nn.MSELoss();\n",
    "    \n",
    "    decay_min = 0.01\n",
    "    decay_const = 0.01\n",
    "    \n",
    "    successes = 0\n",
    "    _gamma = 0.99\n",
    "    # _alpha = 0.5\n",
    "\n",
    "    for i in range(n_ep):\n",
    "        epsilon = decay_min + (1 - decay_min) * np.exp(-decay_const * i)\n",
    "        observation, info = _env.reset()\n",
    "        S = observation\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            A = epsilon_greedy(behaviour_net, S, epsilon)\n",
    "            S_prime, R, terminated, truncated, info = _env.step(A)\n",
    "            done = terminated or truncated\n",
    "            replay_buffer.push(S, A, R, S_prime, done)\n",
    "\n",
    "            S = S_prime\n",
    "            \n",
    "            if len(replay_buffer) > 64:\n",
    "                # sample from replay buffer\n",
    "                state_batch, action_batch, reward_batch, next_state_batch, done_batch = replay_buffer.sample(64)\n",
    "                # feed these values to Q-network, get the Q-values\n",
    "                q_values = behaviour_net(state_batch)\n",
    "                # do magic shit to get Q-value for the action taken\n",
    "                specific_qs = q_values.gather(1, action_batch.unsqueeze(1)).squeeze(1)\n",
    "                # get max q-value from target net\n",
    "                with torch.no_grad():\n",
    "                    specific_target_qs = target_net(next_state_batch).max(1)[0]\n",
    "                # calculate target q-value\n",
    "                target_q_values = reward_batch + _gamma * specific_target_qs * (1 - done_batch)                \n",
    "                #loss\n",
    "                loss = loss_func(specific_qs, specific_target_qs)\n",
    "                #backprop\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                    \n",
    "            if (i % 10) == 0:\n",
    "                target_net.load_state_dict(behaviour_net.state_dict())\n",
    "            \n",
    "                \n",
    "    print(\"Number of successes:\", successes)\n",
    "    return behaviour_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ad12e329-dd16-43f0-ad05-ac21e0008ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pranav\\AppData\\Local\\Temp\\ipykernel_3688\\3155529815.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of successes: 18\n"
     ]
    }
   ],
   "source": [
    "Q_net = DQL(env, 20)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
