{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a8da382-868e-4a1e-9394-d25fec9fa61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # finally get to use pytorch again\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1f7a7ac-7110-4aa2-bc08-d27094324af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "action = env.action_space.sample()\n",
    "\n",
    "def generate_episode(n_ep, env):\n",
    "    for _ in range(n_ep):\n",
    "        observation, info = env.reset()\n",
    "\n",
    "        while True:\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef0738a8-d816-4390-ae34-d6b9be023399",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_network(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim) -> None:\n",
    "        super().__init__()\n",
    "        # note to self: nn.Linear() represents the transformation, not the matrices themselves.\n",
    "        self.il = nn.Linear(in_dim, 50)\n",
    "        self.ol = nn.Linear(50, out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.il(x))\n",
    "        return F.relu(self.ol(x))\n",
    "\n",
    "# shamelessly copied off of \n",
    "class Replay_Memory():\n",
    "    def __init__(self, cap):\n",
    "        self.memory = deque([], maxlen = cap)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(*args)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return np.random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "485cd269-4ff6-46e3-87ac-ffc4a43eb889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(Q_net, S, epsilon):\n",
    "    r = torch.random.random()\n",
    "    if r > epsilon:\n",
    "        return torch.argmax(Q_net.forward(S))\n",
    "    else:\n",
    "        return torch.random.choice([0, 1, 2, 3])\n",
    "\n",
    "def DQL(_env, n_ep):\n",
    "    state_dim = _env.observation_space.shape[0]\n",
    "    action_dim = _env.action_space.n\n",
    "    \n",
    "    buffer_cap = 100000\n",
    "    replay_buffer = Replay_Memory(buffer_cap)\n",
    "    \n",
    "    behaviour_net = Q_network(state_dim, action_dim)\n",
    "    target_net = Q_network(state_dim, action_dim)\n",
    "    optimizer = torch.optim.Adam(behaviour_net.parameters(), lr=0.01)\n",
    "    \n",
    "    decay_min = 0.01\n",
    "    decay_const = 0.01\n",
    "    \n",
    "    successes = 0\n",
    "    _gamma = 0.99\n",
    "    # _alpha = 0.5\n",
    "\n",
    "    for i in range(n_ep):\n",
    "        epsilon = decay_min + (1 - decay_min) * np.exp(-decay_const * i)\n",
    "        observation, info = _env.reset()\n",
    "        S = observation\n",
    "        \n",
    "        if (i % 500) == 0:\n",
    "            target_net.load_state_dict(behaviour_net.state_dict())\n",
    "            \n",
    "        while not done:\n",
    "            A = epsilon_greedy(behaviour_net, S, epsilon)\n",
    "            S_prime, R, terminated, truncated, info = _env.step(A)\n",
    "            done = terminated or truncated\n",
    "            replay_buffer.push(S, A, R, S_prime, done)\n",
    "\n",
    "            S = S_prime\n",
    "            \n",
    "            if len(replay_buffer) > 64:\n",
    "                # sample from replay buffer\n",
    "                state_batch, action_batch, reward_batch, next_state_batch, done_batch = replay_buffer.sample(batch_size)\n",
    "                # feed these values to Q-network, get the Q-values\n",
    "                q_values = behaviour_net(state_batch)\n",
    "                # do magic shit to get Q-value for the action taken\n",
    "                q_values_for_actions = q_values.gather(1, action_batch.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                # get max q-value \n",
    "                with torch.no_grad():\n",
    "                    next_q_values = target_net(next_state_batch).max(1)[0]\n",
    "\n",
    "                # calc target q-value estimation\n",
    "                \n",
    "            loss = (Q_net.forward(S) - target_q) ** 2\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            if terminated or truncated:\n",
    "                if R == 1:\n",
    "                    successes += 1\n",
    "                break\n",
    "                \n",
    "    print(\"Number of successes:\", successes)\n",
    "    return behaviour_net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
